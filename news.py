import requests
from bs4 import BeautifulSoup
from sumy.parsers.plaintext import PlaintextParser
from sumy.summarizers.lsa import LsaSummarizer
import os
import random
import feedparser

# Telegram ë´‡ ì •ë³´
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")  # GitHub Secretsì—ì„œ ê°€ì ¸ì˜´
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")  # GitHub Secretsì—ì„œ ê°€ì ¸ì˜´
TELEGRAM_GROUP_CHAT_ID = os.getenv("TELEGRAM_GROUP_CHAT_ID")

def get_latest_news():
    NEWS_URL = "https://news.ycombinator.com/"  # Hacker News ì˜ˆì‹œ
    
    """Hacker Newsì—ì„œ ìµœì‹  ë‰´ìŠ¤ ì œëª©ê³¼ ë§í¬ ê°€ì ¸ì˜¤ê¸°"""
    response = requests.get(NEWS_URL)
    soup = BeautifulSoup(response.text, "html.parser")

    news_items = []
    for item in soup.select(".titleline a")[:5]:  # ìƒìœ„ 5ê°œ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
        title = item.text
        link = item.get("href")
        news_items.append(f"ğŸ”¹ {title}\nğŸ”— {link}")

    return "\n\n".join(news_items)

def get_latest_korean_news():
    """í•œêµ­ ì£¼ìš” ë‰´ìŠ¤ 5ê°œ ê°€ì ¸ì˜¤ê¸°"""
    url = "https://news.naver.com/main/latestNews.naver"  # ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ í˜ì´ì§€

    # User-Agent ëª©ë¡
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
        "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Mobile/15E148 Safari/604.1",
    ]

    # ëœë¤ User-Agent í—¤ë” ì„¤ì •
    headers = {
        "User-Agent": random.choice(USER_AGENTS)
    }

    # í”„ë¡ì‹œ ë¦¬ìŠ¤íŠ¸
    proxies_list = [
        {"http": "http://5.78.124.240:40000", "https": "http://5.78.124.240:40000"},
        {"http": "http://222.252.194.204:8080", "https": "http://222.252.194.204:8080"},
        {"http": "http://85.215.64.49:80", "https": "http://85.215.64.49:80"},
        {"http": "http://72.10.160.170:5475", "https": "http://72.10.160.170:5475"},
        {"http": "http://50.223.246.23:80", "https": "http://50.223.246.23:80"},
        {"http": "http://50.174.7.159:80", "https": "http://50.174.7.159:80"},
        {"http": "http://41.207.187.178:80", "https": "http://41.207.187.178:80"},
    ]
    
    # ëœë¤ìœ¼ë¡œ í”„ë¡ì‹œ ì„ íƒ
    proxies = random.choice(proxies_list)

    response = requests.get(url, headers=headers)
    #response = requests.get(url, headers=headers, proxies=proxies)
    #response = requests.get(url, headers=headers, proxies=proxies, verify=False)

    #session = requests.Session()
    #response = session.get(url, headers=headers)
    
    if response.status_code != 200:
        print(f"Failed to fetch news: {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")

    # ì£¼ìš” ë‰´ìŠ¤ ì„¹ì…˜ì—ì„œ ë‰´ìŠ¤ ì œëª© ë° ë§í¬ ê°€ì ¸ì˜¤ê¸°
    news_list = []
    headlines = soup.select(".hdline_article_tit a")[:5]  # ìƒìœ„ 5ê°œ ë‰´ìŠ¤
    for headline in headlines:
        title = headline.text.strip()
        link = headline["href"]
        if not link.startswith("http"):
            link = "https://news.naver.com" + link
        news_list.append(f"{title}\n{link}")
    
    return news_list

def fetch_article_content(url):
    """ê¸°ì‚¬ URLì—ì„œ ë³¸ë¬¸ ë‚´ìš©ì„ í¬ë¡¤ë§í•˜ê³  ë¦¬í„´"""
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # ê¸°ì‚¬ ë³¸ë¬¸ì´ ìˆëŠ” íƒœê·¸ë¥¼ ì°¾ì•„ì„œ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜ (ë„¤ì´ë²„, ì—°í•©ë‰´ìŠ¤ ë“± ì›¹ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    # ì˜ˆì‹œ: ì—°í•©ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§
    paragraphs = soup.find_all('div', {'class': 'news_body'})  # ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ div class ì˜ˆì‹œ
    content = ' '.join([para.get_text() for para in paragraphs])

    return content

def summarize_text(text):
    """ê¸°ì‚¬ ë³¸ë¬¸ì„ ìš”ì•½"""
    parser = PlaintextParser.from_string(text, PlaintextParser.from_string(text, text))
    summarizer = LsaSummarizer()
    summary = summarizer(parser.document, 2)  # 2ê°œì˜ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
    summary_text = ' '.join([str(sentence) for sentence in summary])

    return summary_text

def get_latest_rss_news():

    # ì—°í•©ë‰´ìŠ¤ RSS í”¼ë“œ URL
    rss_url = "https://www.yna.co.kr/rss/news.xml"

    """ì—°í•©ë‰´ìŠ¤ RSS í”¼ë“œì—ì„œ ìµœì‹  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°"""
    feed = feedparser.parse(rss_url)  # RSS í”¼ë“œ íŒŒì‹±
    
    # ì „ì²´ í•­ëª© í™•ì¸
    print(f"ì „ì²´ í•­ëª© ìˆ˜: {len(feed.entries)}")

    news_list = []

    for entry in feed.entries[:5]:  # ìƒìœ„ 5ê°œ ê¸°ì‚¬ë§Œ ê°€ì ¸ì˜¤ê¸°
        title = entry.title
        link = entry.link

        # ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§
        article_content = fetch_article_content(link)
        
        # ë³¸ë¬¸ ìš”ì•½
        summary = summarize_text(article_content)
        
        news_list.append(f"**{title}**\n{summary}\n{link}")
        #news_list.append(f"{title}\n{link}")

    print(f"rssë‰´ìŠ¤: {news_list}")

    return "\n\n".join(news_list)

def send_telegram_message(message):
    """Telegram ë´‡ìœ¼ë¡œ ë©”ì‹œì§€ ì „ì†¡"""
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    data = {
        "chat_id": TELEGRAM_GROUP_CHAT_ID,
        "text": message,
        "disable_web_page_preview": True,
        "parse_mode": "Markdown"
    }
    response = requests.post(url, data=data)
    return response.json()

if __name__ == "__main__":
    news = get_latest_rss_news()
    if news:
        send_telegram_message(news)
    else:
        send_telegram_message("âš ï¸ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
